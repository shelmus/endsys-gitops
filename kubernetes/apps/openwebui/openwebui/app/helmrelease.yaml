# https://artifacthub.io/packages/helm/open-webui/open-webui?modal=values
---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: openwebui
spec:
  interval: 1h
  chart:
    spec:
      chart: open-webui
      version: 8.12.2
      sourceRef:
        kind: HelmRepository
        name: openwebui
        namespace: flux-system
  install:
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      retries: 3
  values:
    ollama:
      enabled: false
      image:
        repository: ollama/ollama
        pullPolicy: IfNotPresent
        tag: "0.12.7-rocm"
      nodeSelector:
        kubernetes.io/hostname: "kube3"
      fullnameOverride: "cleon"
      # -- Example Ollama configuration with nvidia GPU enabled, automatically downloading a model, and deploying a PVC for model persistence
      ollama:
        gpu:
          enabled: true
          type: 'amd'
          # number: 1
        models:
          pull:
            - gpt-oss:latest
          run:
            - gpt-oss:latest
      # runtimeClassName: amd
      persistentVolume:
        enabled: true
        # volumeName: "example-pre-existing-pv-created-by-smb-csi"
      resources:
        limits:
          amd.com/gpu: 1
          # memory: 16Gi
          # cpu: "8"
        requests:
          amd.com/gpu: 1
          # memory: 8Gi
          # cpu: "4"
      extraEnv:
          - name: HSA_OVERRIDE_GFX_VERSION
            value: "10.3.0"
          - name: OLLAMA_KEEP_ALIVE
            value: "24h"   # keep model cached between calls
          - name: OLLAMA_NUM_PARALLEL
            value: "4"
          # - name: OMP_NUM_THREADS
          #   value: "8"     # match CPU cores allocated
          - name: OMP_WAIT_POLICY
            value: "PASSIVE"
          - name: HSA_ENABLE_SDMA
            value: "0"     # reduce PCIe DMA stalls on some GPUs
          - name: HSA_FORCE_FINE_GRAIN_PCIE
            value: "1"     # ensure fine-grained GPU/CPU sharing
          - name: AMD_LOG_LEVEL
            value: "3"
          - name: OLLAMA_DEBUG
            value: "1"
    persistence:
      enabled: true
      size: 20Gi
    nodeSelector:
      kubernetes.io/hostname: "kube3"
    sso:
      enabled: false
      enableSignup: true
      mergeAccountsByEmail: true
      enableRoleManagement: false
      enableGroupManagement: false
      oidc:
        enabled: true
        clientId: "openwebui"
        # clientSecret: ""
        clientExistingSecret: openwebui-sso-client-secret
        clientExistingSecretKey: client_secret
        providerUrl: "https://auth.endsys.cloud/.well-known/openid-configuration"
        providerName: "Authelia"
        scopes: "openid email groups profile"
    # -- Env vars added to the Open WebUI deployment. Most up-to-date environment variables can be found here: https://docs.openwebui.com/getting-started/env-configuration/
    extraEnvVars:
      # - name: OLLAMA_BASE_URL
      #   value: http://cleon-ollama.ollama.svc.cluster.local:11434
      # -- Default API key value for Pipelines. Should be updated in a production deployment, or be changed to the required API key if not using Pipelines
      # - name: OPENAI_API_KEY
      #   value: "0p3n-w3bu!"
      # valueFrom:
      #   secretKeyRef:
      #     name: pipelines-api-key
      #     key: api-key
      - name: OPENAI_API_KEY
        valueFrom:
          secretKeyRef:
            name: openai-api-key
            key: api-key
      - name: OLLAMA_DEBUG
        value: "1"
